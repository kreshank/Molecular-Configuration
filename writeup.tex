\documentclass[11pt,onecolumn]{article}
\usepackage{amssymb, amsmath, amsthm,graphicx, paralist,algpseudocode,algorithm,cancel,url,color}
\usepackage{sectsty}
\usepackage{fancyvrb}
\usepackage{mathrsfs}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{booktabs}
\usepackage[table]{xcolor}
\usepackage{tikz}
% \usepackage[framed,numbered,autolinebreaks,useliterate]{mcode}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{mathtools}

\newcommand{\bvec}[1]{\mathbf{#1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Rn}{\R^{n\times n}}
\newcommand{\Rmn}{\R^{m\times n}}
\newcommand{\Cn}{\C^{n\times n}}
\newcommand{\Cmn}{\C^{m\times n}}
\newcommand{\cO}{\mathcal{O}}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\trace}{trace}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\vspan}{span}
\DeclarePairedDelimiter{\norm}{\|}{\|}
\DeclarePairedDelimiter{\abs}{|}{|}
\DeclarePairedDelimiter{\PAREN}{(}{)}
\sectionfont{\Large\sc}
\subsectionfont{\sc}
\usepackage[margin=1 in]{geometry}
\begin{document}
\noindent
\textsc{\Large Numerical analysis: P2}\\
Students: Bodong Liu (bl576), Ryan Wu (123123)\\
Date: May 3, 2025
\begin{center}\rule{1\linewidth}{0.5pt}\end{center}

\subsection*{Question 1:}
\noindent
\begin{enumerate}
    \item 
    For gradient descent:
    \begin{itemize}
        \item 
        Update rule:\\
        $$
        X_{k+1}=X_k - \alpha_k \nabla f(X_k)
        $$
        \item 
        Line search:\\
        We use a simple backtracking rule: start with $\alpha = \alpha_0$ and repeatedly muiply by a factor $\beta < 1$ until
        $$
        f(X_k - \alpha \nabla f(X_k)) \leq f(X_k) + c \alpha \nabla f(X_k)^\top (-\nabla f(X_k))
        $$
        with typical parameters $\beta = 0.5$ and $c=10^{-4}$.

        \item 
        Stopping criteria:\\
        We check the gradient-norm and terminate if the gradient is too small:
        $$
        \| \nabla f(X) \| \leq \text{tol}.
        $$
        where we set the tolerance threshold to be $10^{-8}$ by default.
    \end{itemize}
    
    \item 
    For BFGS quasi‑Newton:
    \begin{itemize}
        \item 
        Inverse‐Hessian update:\\
        In the reduced $\R^{3(N-1)}$ subspace (fixing one atom due to translational symmetry), we maintain $H_k^{-1}$ and form the search direction
        $$
        s_k = -H^{-1}_k g_k, \quad g_k = \nabla f(X_k)_{\text{free}}.
        $$
        After a line seaerch $\alpha_k$ (same backtracking as above), we set
        $$
        x_{k+1} = x_k+ \alpha_k s_k, \quad
        y_k = g_{k+1}, \quad
        \rho_k = \frac{1}{y_k^\top s_k}
        $$
        and then we update
        $$
        H_{k+1}^{-1}=(I-\rho_k s_k y_k^\top)H_k^{-1} (I-\rho_k y_k s_k^\top)+\rho_k s_k s_k^\top
        $$
        \item 
        Stopping criteria:\\
        We check both the delta energy and gradient-norm, and terminate if either one of them is too small:
        $$
        \left[ \| \nabla f(X) \| \leq \text{g\_tol} \right] \, \vee \, \left[|f(X_{k+1}) - f(X_k)| \leq \text{energy\_tol}\right].
        $$
        where we set both tolerance thresholds to be $10^{-8}$ by default.\\
        We stop if the energy change is too small because once the goal has plateaued, it is unnecessary to keep performing Hessian updates. Similar logic applies to the gradient norm check.\\
        Also note that in the specific implementation of our code, we check delta energy before checking the gradient. This small detail is beneficial because we want to catch the "we are done" as early as possible--if the quasi-Newton step did not move the objective at all (within the tolerance threshold), there is no point keep computing or checking 
    \end{itemize}


    
\end{enumerate}



\end{document}
